{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Class 3: Intro to Machine Learning Models\n",
        "There are many types of machine learning models.  \n",
        "We learned linear models yesterday and we will learn tree-based models today. <br>\n",
        "Day 4 will be neural networks."
      ],
      "metadata": {
        "id": "RvnZJbq_uma4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iypnAlsoui_2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in our dataset\n",
        "filename = 'https://raw.githubusercontent.com/sayhellojoel/grade78pythonmath/main/Data/kids%20anonymous%20data.csv'\n",
        "df = pd.read_csv(filename)"
      ],
      "metadata": {
        "id": "XXjgAEQYvAGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the first 5 lines from our dataset\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "zSXNWyGRvDHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "This data preparation is used for all models below."
      ],
      "metadata": {
        "id": "f-pfU90rq-LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing our data for the model\n",
        "inputs = ['Age Decimal', 'FOOT LENGTH', 'INDEX FINGER'] # these are the variables we want to use as inputs\n",
        "X = df[inputs]\n",
        "y = df['HEIGHT']\n",
        "\n",
        "seed = 1\n",
        "\n",
        "# Splitting the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
      ],
      "metadata": {
        "id": "A5y7VNGfkI8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Linear Model (with Multiple Variables)"
      ],
      "metadata": {
        "id": "CofeLdawqRfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Creating the linear regression model\n",
        "model_linear = LinearRegression()\n",
        "\n",
        "# Training the model\n",
        "model_linear.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3t3Fkg5Jp3uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's inspect the slopes for each variable\n",
        "# this tells us how much each variable affects height\n",
        "pd.DataFrame({'Variables': inputs, 'Slopes': model_linear.coef_})"
      ],
      "metadata": {
        "id": "oBZOzCuqq6zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's visualize the slopes for each\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "fig, ax = plt.subplots(figsize=(12, 4)) # this allows us to plot all 3 together\n",
        "PartialDependenceDisplay.from_estimator(model_linear, X_train, features=inputs, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Pgf7hLer8lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Decision Tree Model"
      ],
      "metadata": {
        "id": "fDiWn3B60AiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "\n",
        "# Creating the Decision Tree model\n",
        "model_dt = DecisionTreeRegressor(random_state=seed, max_depth=2)\n",
        "\n",
        "# Training the model\n",
        "model_dt.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ke1EtXHSvvRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the tree\n",
        "plt.figure(figsize=(10,6))\n",
        "plot_tree(model_dt, filled=True, feature_names=inputs, impurity=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QiPhSslnv-hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's visualize each variable's affect on height\n",
        "# Does INDEX FINGER affect anything?\n",
        "fig, ax = plt.subplots(figsize=(12, 4)) # this allows us to plot all 3 together\n",
        "PartialDependenceDisplay.from_estimator(model_dt, X_train, features=inputs, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-tL3rM-cuoeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Random Forest Model"
      ],
      "metadata": {
        "id": "IFwnbDuf0Ey8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model_rf = RandomForestRegressor(random_state=seed, max_depth=2, n_estimators=100) # makes a forest with 100 trees\n",
        "model_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xQ0rZcShwC-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'Variables': inputs, 'Importance': model_rf.feature_importances_})"
      ],
      "metadata": {
        "id": "h0k5jo330qKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's visualize each variable's affect on height\n",
        "# Does INDEX FINGER affect anything?\n",
        "fig, ax = plt.subplots(figsize=(12, 4)) # this allows us to plot all 3 together\n",
        "PartialDependenceDisplay.from_estimator(model_rf, X_train, features=inputs, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2vaGn18mucSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the trees\n",
        "tree_rf1 = model_rf.estimators_[0]  # Change the index to view different trees\n",
        "tree_rf2 = model_rf.estimators_[49]  # Change the index to view different trees\n",
        "tree_rf3 = model_rf.estimators_[99]  # Change the index to view different trees\n",
        "\n",
        "plt.figure(figsize=(18,6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plot_tree(tree_rf1, filled=True, feature_names=inputs, impurity=False)\n",
        "plt.title('First Tree in Random Forest')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plot_tree(tree_rf2, filled=True, feature_names=inputs, impurity=False)\n",
        "plt.title('50th Tree in Random Forest')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plot_tree(tree_rf3, filled=True, feature_names=inputs, impurity=False)\n",
        "plt.title('Last Tree in Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1dUmo_ch0Qgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Models\n",
        "If we were predicting a discrete variable, we can use <b>accuracy</b>. Accuracy is the percent of time the prediction is correct.  \n",
        "But we are predicting a continuous variable. So instead we can look at the average our prediction is off by.  \n",
        "This is called Mean Absolute Error (MAE).  "
      ],
      "metadata": {
        "id": "TiDx8g1-FbMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# testing our model's predictions with the test data\n",
        "predictions_linear = model_linear.predict(X_test)\n",
        "predictions_dt = model_dt.predict(X_test)\n",
        "predictions_rf = model_rf.predict(X_test)\n",
        "\n",
        "# Calculating Mean Absolute Error (MAE)\n",
        "mae_linear = mean_absolute_error(y_test, predictions_linear).round(1)\n",
        "mae_dt = mean_absolute_error(y_test, predictions_dt).round(1)\n",
        "mae_rf = mean_absolute_error(y_test, predictions_rf).round(1)\n",
        "\n",
        "names_list = ['Linear', 'Decision Tree', 'Random Forest']\n",
        "error_list = [mae_linear, mae_dt, mae_rf]\n",
        "\n",
        "pd.DataFrame({'Models': names_list, 'Error': error_list})"
      ],
      "metadata": {
        "id": "zyIp74EJ0hEw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}